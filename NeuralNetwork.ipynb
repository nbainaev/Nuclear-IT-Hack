{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LLeQ6iUKFg9"
      },
      "outputs": [],
      "source": [
        "!pip install yolov5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "isqvbm1DXPbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "uHGn0xPEbVte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import yolov5\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image, ImageDraw\n",
        "import time\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "#from transformers import AutoModelForImageClassification, TrainingArguments, Trainer, AutoImageProcessor, DefaultDataCollator\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "1kBEGiUXKMhs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt.neuro"
      ],
      "metadata": {
        "id": "NT3C12LDWHy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "HXlFlgVMdJzy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9pJsQ4GdJqG",
        "outputId": "8981a0b2-f515-4915-801a-e885dbc4b1bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive//Nuclear IT Hack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyROlweMdTlT",
        "outputId": "1d7c7d6f-8872-43ea-a32e-5b3926a9d623"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Nuclear IT Hack\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "NyKTKt-rS1wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset_hack"
      ],
      "metadata": {
        "id": "85we8EQUtncL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Считывание исходного датасета"
      ],
      "metadata": {
        "id": "tHuQRBUXeUpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smokers_path = sorted(list(map(str, Path(\"./smokers\").glob(\"*.jpg\"))))\n",
        "non_smokers_path = sorted(list(map(str, Path(\"./non_smokers\").glob(\"*.jpg\"))))\n",
        "labels = []\n",
        "for i, path in enumerate(smokers_path):\n",
        "    with Image.open(path) as img:\n",
        "        if i == 0:\n",
        "            img = img.resize((640, 640))\n",
        "            data = np.asarray(img).reshape(1, 640,640,3)\n",
        "            labels.append(\"smoker\")\n",
        "        else:\n",
        "            img = img.resize((640, 640))\n",
        "            data = np.concatenate((data, np.asarray(img).reshape(1,640,640,3)), axis=0)\n",
        "for path in non_smokers_path:\n",
        "    with Image.open(path) as img:\n",
        "        img = img.resize((640, 640))\n",
        "        data = np.concatenate((data, np.asarray(img).reshape(1, 640,640,3)), axis=0)\n",
        "        labels.append(\"non_smoker\")"
      ],
      "metadata": {
        "id": "eM1QQFnhdDDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_MODES = [\"train\", \"val\", \"test\"]\n",
        "RESCALE_SIZE = 299\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "8xsIv9HfuWoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_smoke = list(map(str, Path(\"./smokers\").glob(\"*.jpg\")))\n",
        "file_path_non = list(map(str, Path(\"./people\").glob(\"*.jpg\")))\n",
        "train_val_labels = [1] * len(file_path_smoke) + [0] * len(file_path_non)\n",
        "file_path = file_path_smoke + file_path_non"
      ],
      "metadata": {
        "id": "ZKdr3_YKvLYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files_smoke = list(map(str, Path(\"./dataset/smokers\").glob(\"*.jpg\")))\n",
        "test_files_non = list(map(str, Path(\"./dataset/people\").glob(\"*.jpg\")))\n",
        "test_files = test_files_smoke + test_files_non\n",
        "test_labels = [1] * len(test_files_smoke) + [0] * len(test_files_non)"
      ],
      "metadata": {
        "id": "xH54Y5sAMPRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path += test_files\n",
        "train_val_labels += test_labels"
      ],
      "metadata": {
        "id": "sJ_eRjS0JxR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmokersDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Датасет с картинками, который паралельно подгружает их из папок\n",
        "    производит скалирование и превращение в торчевые тензоры\n",
        "    \"\"\"\n",
        "    def __init__(self, files, mode, labels):\n",
        "        super().__init__()\n",
        "        # список файлов для загрузки\n",
        "        self.files = files\n",
        "        # режим работы\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode not in DATA_MODES:\n",
        "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
        "            raise NameError\n",
        "\n",
        "        self.len_ = len(self.files)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_\n",
        "\n",
        "    def load_sample(self, file):\n",
        "        image = Image.open(file)\n",
        "        image.load()\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
        "        if self.mode != \"test\":\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "            ])\n",
        "            x = self.load_sample(self.files[index])\n",
        "            x = self._prepare_sample(x)\n",
        "            x = np.array(x / 255, dtype='float32')\n",
        "            if list(x.shape) != [RESCALE_SIZE, RESCALE_SIZE, 3]:\n",
        "                print(self.files[index], x.shape)\n",
        "            x = transform(x)\n",
        "        else:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "            x = self.load_sample(self.files[index])\n",
        "            x = self._prepare_sample(x)\n",
        "            x = np.array(x / 255, dtype='float32')\n",
        "\n",
        "        label = self.labels[index]\n",
        "        return x, label\n",
        "\n",
        "    def _prepare_sample(self, image):\n",
        "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
        "        return np.array(image)"
      ],
      "metadata": {
        "id": "VlSkoEi3uPuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(file_path, train_val_labels, test_size=0.25, \\\n",
        "                                          stratify=train_val_labels)\n",
        "\n",
        "train_files += test_files\n",
        "train_labels += test_labels"
      ],
      "metadata": {
        "id": "-8dn_kCRvfZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Подготовка модели детекции"
      ],
      "metadata": {
        "id": "tkVO6lJWears"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = yolov5.load('yolov5l6.pt')"
      ],
      "metadata": {
        "id": "T3xy8vGvKS5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.conf = 0.25  # NMS confidence threshold\n",
        "model.iou = 0.45  # NMS IoU threshold\n",
        "model.agnostic = False  # NMS class-agnostic\n",
        "model.multi_label = False  # NMS multiple labels per box\n",
        "model.max_det = 1000  # maximum number of detections per image\n",
        "model.classes = 0"
      ],
      "metadata": {
        "id": "RGJZXA0oLE_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tic = time.time()\n",
        "out = model(\"/content/example1.jpg\")\n",
        "toc = time.time()\n",
        "print(f\"Prosses time: {(toc - tic) * 1000} ms\")"
      ],
      "metadata": {
        "id": "kdZ-7oUQLMLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab584c7-be13-4329-f06c-ab644dfa43a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prosses time: 267.2383785247803 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crop_img = out.crop(save=False)"
      ],
      "metadata": {
        "id": "gClpZkgCfsTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.xyxy[0][0][:4].reshape(1,4).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KOA24R_pJKg",
        "outputId": "2ece7a4b-02f9-4c16-d7a6-1207ac4c0266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bbox = draw_bounding_boxes(torch.tensor(np.moveaxis(data[0], 2, 0)), out.xyxy[0][0][:4].reshape(1,4))"
      ],
      "metadata": {
        "id": "xBlZIOf5oYT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SECOWmIlqt0U",
        "outputId": "b8cc0190-9af1-410b-ddd8-a3328f08c758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 640, 640])"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.fromarray(np.moveaxis(np.array(bbox), 0, 2))"
      ],
      "metadata": {
        "id": "0Dmr90KIqjzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(data.shape[0]):\n",
        "    out = model(data[i])\n"
      ],
      "metadata": {
        "id": "oBCtmyzQnyLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Тренировка классификатора"
      ],
      "metadata": {
        "id": "N0ZRa0GEee-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e82818b9698b4f74a55c75262027dda8",
            "be730fb8ad114b27a2451b7eeddea0c0",
            "22f28f417e994c508d280e899e22dbb7",
            "6301848bfff34211aefcec515ff1149b",
            "f7e13dd5a8f1457986234d848ecefb1d",
            "081471be805b42d8a2a3e58ea0895d8a",
            "df3e58c63f3443dfbc56bff32e4dfa7b",
            "4bfa12ccefcf4d3b95452d152f145d99",
            "6650fa53efed4c82b49571129560126b",
            "5fac2f2904a148f1825b46b81c0a68e1",
            "2d87f12feb1e46fea208dfeb13a8c66b",
            "f39955a1c3fe4d2785b1be7c18895a4e",
            "ec7486c5b6a04ca29c287d7d4993c9da",
            "3e490eef97fc4dfbb8aee9bb7623aed9",
            "750df290a0be44389102fe539597c8d7",
            "dc95c74e6aad4574b2ae0a8848c3554a",
            "647d232c5e68481e94b353d19e942c2c",
            "e35f67b2187d45be9c3fbde7d96a8a8e",
            "1a225a1340024a3f898e72f8d3174ec4",
            "9874a4e7275745049e881b9bb1b668bc",
            "5b10ef482b3444f5a2b5710a6abb6811",
            "d0a1957aa28e4709a8a1e0626b288818"
          ]
        },
        "id": "xl6bE-cFaEb1",
        "outputId": "5f629ec9-0e13-4ff9-e4f9-e630f38f4276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e82818b9698b4f74a55c75262027dda8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f39955a1c3fe4d2785b1be7c18895a4e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DefaultDataCollator()"
      ],
      "metadata": {
        "id": "2mK_sn8aaevm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['smoker', 'non_smoker']\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label"
      ],
      "metadata": {
        "id": "QJGMKDORaRxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "size = (\n",
        "    image_processor.size[\"shortest_edge\"]\n",
        "    if \"shortest_edge\" in image_processor.size\n",
        "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        ")\n",
        "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
      ],
      "metadata": {
        "id": "HTStQ4hrb4Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_model = AutoModelForImageClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    from_pt=False\n",
        ")"
      ],
      "metadata": {
        "id": "SfTGSbmIW72M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_model.init_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92eaIfbXhlmi",
        "outputId": "b024befa-3254-4788-eb7e-27ea3a6c5bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=2, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_food_model\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=food[\"train\"],\n",
        "    eval_dataset=food[\"test\"],\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "RenE1gd1ccCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqgsTyPHhENA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "TOsD3Z34cekU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#InceptionV3"
      ],
      "metadata": {
        "id": "iqaVX2j5kyGH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2mk7MNtcUhJ"
      },
      "source": [
        "def fit_epoch(model, train_loader, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_data = 0\n",
        "\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs).logits\n",
        "        #outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_data += inputs.size(0)\n",
        "\n",
        "    train_loss = running_loss / processed_data\n",
        "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
        "    return train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_CD9--hcUjs"
      },
      "source": [
        "def eval_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_size = 0\n",
        "\n",
        "    model.eval()\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data).detach().cpu().numpy()\n",
        "        processed_size += inputs.size(0)\n",
        "        # print(f\"labels: {labels}\")\n",
        "        # print(f\"preds: {preds}\")\n",
        "    val_loss = running_loss / processed_size\n",
        "    val_acc = running_corrects / processed_size\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaxYIwB3cUmX"
      },
      "source": [
        "def train(train_files, val_files, model, epochs, batch_size):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    history = []\n",
        "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
        "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
        "    with tqdm.tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
        "        opt = torch.optim.Adam(model.parameters())\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
        "            print(\"loss\", train_loss)\n",
        "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "            pbar_outer.update(1)\n",
        "            tqdm.tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
        "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
        "            torch.cuda.empty_cache()\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6G7qbYqcUpL"
      },
      "source": [
        "def predict(model, test_loader):\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "\n",
        "        for inputs, label in test_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "            labels += label\n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
        "    return probs, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_inception_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44417e42-720c-45f6-e882-da8d78ca522c",
        "id": "T5faocB8xtIG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_inception_v3.fc = nn.Linear(2048, 2)"
      ],
      "metadata": {
        "id": "ZpUVmZ0vxtIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights(\"Inception_weights\", my_inception_v3)"
      ],
      "metadata": {
        "id": "FqrfIncs3doW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for params in list(my_inception_v3.parameters())[:-25]:\n",
        "    params.requires_grad = False"
      ],
      "metadata": {
        "id": "hVW1efJ6xtIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_inception_v3.to(DEVICE)\n",
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm0VKGoiVlFs",
        "outputId": "9460a95b-b0e6-40c5-dffe-28aa981cfb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = SmokersDataset(val_files, mode='val', labels=val_labels)\n",
        "\n",
        "train_dataset = SmokersDataset(train_files, mode='train', labels=train_labels)"
      ],
      "metadata": {
        "id": "PNHCD_T0vzMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDXoR8PIdfLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c644a6a2-2ba1-414f-e8f6-c1803677a994"
      },
      "source": [
        "history = train(train_dataset, val_dataset, model=my_inception_v3, epochs=5, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.438278631362694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch:  20%|██        | 1/5 [00:14<00:59, 14.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 train_loss: 0.4383     val_loss 0.5888 train_acc 0.7881 val_acc 0.8421\n",
            "loss 0.1173021195345367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch:  40%|████      | 2/5 [00:30<00:46, 15.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 train_loss: 0.1173     val_loss 0.6998 train_acc 0.9570 val_acc 0.8553\n",
            "loss 0.062326968709216606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch:  60%|██████    | 3/5 [00:46<00:30, 15.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 train_loss: 0.0623     val_loss 0.7510 train_acc 0.9834 val_acc 0.8289\n",
            "loss 0.021585328837498925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch:  80%|████████  | 4/5 [01:01<00:15, 15.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 train_loss: 0.0216     val_loss 0.8145 train_acc 0.9967 val_acc 0.7763\n",
            "loss 0.00969709816160581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 100%|██████████| 5/5 [01:16<00:00, 15.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 train_loss: 0.0097     val_loss 0.8150 train_acc 1.0000 val_acc 0.7632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ryD_9yFdfNr"
      },
      "source": [
        "loss, acc, val_loss, val_acc = zip(*history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpQDWGkfdfQ5"
      },
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(15, 9))\n",
        "ax[0].plot(loss, label=\"train_loss\")\n",
        "ax[0].plot(val_loss, label=\"val_loss\")\n",
        "ax[0].legend(loc='best')\n",
        "ax[0].set_xlabel(\"epochs\")\n",
        "ax[0].set_ylabel(\"loss\")\n",
        "\n",
        "ax[1].plot(acc, label=\"train_acc\")\n",
        "ax[1].plot(val_acc, label=\"val_acc\")\n",
        "ax[1].legend(loc='best')\n",
        "ax[1].set_xlabel(\"epochs\")\n",
        "ax[1].set_ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SmokersDataset(test_files, mode=\"test\", labels=test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "XlQmqmm3Npsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "probs_ims, true_labels = predict(my_inception_v3, val_dataloader)\n",
        "y_pred = np.argmax(probs_ims,-1)"
      ],
      "metadata": {
        "id": "_J1aHQR1eEdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = np.sum(np.array(true_labels) == np.array(y_pred)) / len(y_pred)\n",
        "correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGXGp-XZei2e",
        "outputId": "6e0d9e15-60d8-4bae-f35a-ca8495ee14dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8947368421052632"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKYhxM-TPyC0",
        "outputId": "c852e578-87c7-474b-fb75-76acd0ed9bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(true_labels, y_pred, average='micro')\n",
        "\n",
        "print(\"F1-оценка:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95OgVOCHeVMO",
        "outputId": "10847631-f1f9-4997-8939-7a76cff43806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-оценка: 0.8947368421052632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AlexNet"
      ],
      "metadata": {
        "id": "UGdtTo_Eh6oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_alex = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfRev-YOh8o6",
        "outputId": "5bec6636-2afc-4365-fb08-9844bb5a4c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_alex"
      ],
      "metadata": {
        "id": "I03xFscVpv0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_alex.classifier[4] = nn.Linear(4096, 2048)\n",
        "my_alex.classifier[6] = nn.Linear(2048, 2)"
      ],
      "metadata": {
        "id": "hA-gW0yypsyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(my_alex.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_m8mpwYqNbq",
        "outputId": "346c78b0-a8a4-4fdb-ee58-084131982630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for params in list(my_inception_v3.parameters())[:-3]:\n",
        "    params.requires_grad = False"
      ],
      "metadata": {
        "id": "JddB_HWWpsyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_alex.to(DEVICE)\n",
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d9a0dd-5353-4bb1-e554-330d1d13e871",
        "id": "N4bxUtMTpsyX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR0mSz98psyX"
      },
      "source": [
        "history = train(train_dataset, val_dataset, model=my_alex, epochs=10, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LxiH249psyX"
      },
      "source": [
        "loss, acc, val_loss, val_acc = zip(*history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHuKvqLtpsyX"
      },
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(15, 9))\n",
        "ax[0].plot(loss, label=\"train_loss\")\n",
        "ax[0].plot(val_loss, label=\"val_loss\")\n",
        "ax[0].legend(loc='best')\n",
        "ax[0].set_xlabel(\"epochs\")\n",
        "ax[0].set_ylabel(\"loss\")\n",
        "\n",
        "ax[1].plot(acc, label=\"train_acc\")\n",
        "ax[1].plot(val_acc, label=\"val_acc\")\n",
        "ax[1].legend(loc='best')\n",
        "ax[1].set_xlabel(\"epochs\")\n",
        "ax[1].set_ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_loCQ40CSKm",
        "outputId": "a1223246-4e37-4c2b-c9ca-015feeb13eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = list(range(val_dataset.len_))\n",
        "imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n",
        "actual_labels = [val_dataset[id][1] for id in idxs]\n",
        "probs_ims = predict(my_alex, imgs)\n",
        "y_pred = np.argmax(probs_ims,-1)"
      ],
      "metadata": {
        "id": "MEc9L6IxpsyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = np.sum(np.array(actual_labels) == np.array(y_pred)) / len(y_pred)\n",
        "correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674dc01c-cec1-43f7-e05c-361fcc7a4fee",
        "id": "Yq-VijnwpsyY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7608695652173914"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(actual_labels, y_pred, average='micro')\n",
        "\n",
        "print(\"F1-оценка:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a30da9-5e4a-4d83-ab97-f3fe0bf362ae",
        "id": "JsY7-HQOpsyY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-оценка: 0.7608695652173914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Инференс"
      ],
      "metadata": {
        "id": "x2BebxIZhCDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classificator = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu26xvm-0xQN",
        "outputId": "af513e77-d3f5-4e68-8770-af704f4450a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Использовать model.parameters()"
      ],
      "metadata": {
        "id": "pCAaKtTY7BCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_weights(name, model):\n",
        "    if not os.path.exists(name):\n",
        "        os.mkdir(name)\n",
        "    for i, params in enumerate(model.parameters()):\n",
        "        torch.save(params, f\"{name}/layer_{i}.pt\")\n",
        "\n",
        "def load_weights(dir_name, model):\n",
        "    paths = sorted(list(map(str, Path(f\"./{dir_name}\").glob(\"*.pt\"))))\n",
        "    for i, params in enumerate(model.parameters()):\n",
        "        params = torch.load(paths[i])"
      ],
      "metadata": {
        "id": "XOwewLb25HZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_weights(\"Inception_weights\", my_inception_v3)"
      ],
      "metadata": {
        "id": "7Qr8Rq0AHy8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights(\"Inception_weights\", classificator)"
      ],
      "metadata": {
        "id": "0kTvlc4LEa2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = yolov5.load('yolov5s.pt')\n",
        "\n",
        "detector.conf = 0.25  # NMS confidence threshold\n",
        "detector.iou = 0.45  # NMS IoU threshold\n",
        "detector.agnostic = False  # NMS class-agnostic\n",
        "detector.multi_label = False  # NMS multiple labels per box\n",
        "detector.max_det = 30  # maximum number of detections per image\n",
        "detector.classes = 0\n",
        "\n",
        "classificator  = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=False)\n",
        "classificator.fc = nn.Linear(2048, 2)\n"
      ],
      "metadata": {
        "id": "ZSDW0_wDhEtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e48a64e-7523-4ef2-95ad-1628f7c6fbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for params in list(classificator.parameters())[:-7]:\n",
        "    params.requires_grad = False"
      ],
      "metadata": {
        "id": "EEQmH4eaWGYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_image = Image.open(val_files[1])\n",
        "example_image"
      ],
      "metadata": {
        "id": "ytsjXSSAjusE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_image = Image.open(train_files[1])\n",
        "\n",
        "result = detector(example_image)"
      ],
      "metadata": {
        "id": "DBW8HrjRkJvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_weights(dir_name, model):\n",
        "    paths = sorted(list(map(str, Path(f\"./{dir_name}\").glob(\"*.pt\"))))\n",
        "    for i, params in enumerate(model.parameters()):\n",
        "        params = torch.load(paths[i])"
      ],
      "metadata": {
        "id": "Y9PTrOkAXry4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = yolov5.load('yolov5s.pt')\n",
        "\n",
        "detector.conf = 0.25  # NMS confidence threshold\n",
        "detector.iou = 0.45  # NMS IoU threshold\n",
        "detector.agnostic = False  # NMS class-agnostic\n",
        "detector.multi_label = False  # NMS multiple labels per box\n",
        "detector.max_det = 30  # maximum number of detections per image\n",
        "detector.classes = 0\n",
        "\n",
        "classificator  = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=False)\n",
        "classificator.fc = nn.Linear(2048, 2)"
      ],
      "metadata": {
        "id": "QGQofwxWXhGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_one_sample(model, inputs, device=DEVICE):\n",
        "    \"\"\"Предсказание, для одной картинки\"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = inputs.to(device)\n",
        "        model.eval()\n",
        "        logit = model(inputs).cpu()\n",
        "        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n",
        "        max_prob = np.max(probs)\n",
        "        y_pred = np.argmax(probs,-1)\n",
        "    return y_pred, max_prob"
      ],
      "metadata": {
        "id": "Y5HB32u6mfVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analize(example_image):\n",
        "    prediction = []\n",
        "    probability = []\n",
        "    bboxes = []\n",
        "    result = detector(example_image)\n",
        "    result = result.crop(save=False)\n",
        "    n_all_people = len(result)\n",
        "    for imgs in result:\n",
        "        if float(imgs['conf'].cpu().numpy()) >= 0.85:\n",
        "            bbox = imgs['box']\n",
        "            bbox = [float(val.cpu().numpy()) for val in bbox]\n",
        "            example_image1 = example_image.crop(bbox)\n",
        "            example_image1 = example_image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
        "            example_image1 = np.array(np.array(example_image1) / 255, dtype='float32')\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "            example_image1 = transform(example_image)\n",
        "\n",
        "            pred, prop = predict_one_sample(classificator.to(DEVICE), example_image1.unsqueeze(0), device=DEVICE)\n",
        "            if pred[0] == 1:\n",
        "                bboxes.append(bbox)\n",
        "                prediction.append(pred[0])\n",
        "                probability.append(prop)\n",
        "\n",
        "    #torchvision.utils.draw_bounding_boxes(torch.tensor(np.array(example_image)), boxes=bboxes, labels=[f\"smoker_{i}; p: {prob}\" for i, prob in enumerate(probability)])\n",
        "    draw = ImageDraw.Draw(example_image)\n",
        "    for bbox in bboxes:\n",
        "        draw.rectangle(tuple(bbox), outline=\"red\", width=3)\n",
        "\n",
        "    return example_image, n_all_people, len(prediction)"
      ],
      "metadata": {
        "id": "jdBI4NFplzjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"/content/ex1.jpg\")\n",
        "image, all_people, n_smokers, pred, prop = analize(image)\n",
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "6-EjuEpLQ4tO",
        "outputId": "9ef7d900-9741-4d15-e6a7-67b0cc14042a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Only grayscale and RGB images are supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-791ff1fc3de4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/ex1.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_people\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_smokers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-68350dd9a805>\u001b[0m in \u001b[0;36manalize\u001b[0;34m(example_image)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mprobability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_bounding_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"smoker_{i}; p: {prob}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36mdraw_bounding_boxes\u001b[0;34m(image, boxes, labels, colors, fill, width, font, font_size)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pass individual images, not batches\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only grayscale and RGB images are supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Only grayscale and RGB images are supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = np.sum(np.array(test_labels) == np.array(prediction)) / len(prediction)\n",
        "correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RPeOV9AUple",
        "outputId": "b89e257f-fc1e-4b9e-fbf5-7e2b26890406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38.0"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels[20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X976hj_IDBgc",
        "outputId": "a35ec233-548e-4cf0-9574-577efeb6f996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dksGKOyLzsp",
        "outputId": "73b5ad6f-d2bd-49df-c260-6e6637579bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amPA8imv6NB3",
        "outputId": "156aa917-f780-4735-a69c-757d411abdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.83096653]"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    }
  ]
}